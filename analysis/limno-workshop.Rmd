---
title: "Intro to R for Limnology"
author: "Cory Sauve"
date: "10/01/2020"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: yes
  html_document:
    number_sections: yes
    theme: yeti
    toc: true
    toc_float: true
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 4, fig.height = 4, fig.align = "center")
```

<style type="text/css">

h1.title {
  text-align: center;
  font-weight: bold;
}
h4.author {
  text-align: center;
}
h4.date { 
  text-align: center;
}
</style>

\newpage 

# Welcome! 

<br>

Welcome to the written part of visualizing lake data in R! Everything covered in the lecture videos is covered here in much more detail. You'll also find information of how to install and set up R and RStudio, getting help, and additional R resources. 

<br>

## Workshop Contents

* **Getting Started** 
  + Covers how to install the required software and how to organize everything you'll need
  
* **Data Analysis Crash Course**
  + Reviews some R basics and most of the functions you'll be using to create your figures 

* **On to University Lake**
  + Covers how to import, manipulate, and visualize your University Lake Data

* **Troubleshooting**
  + Provides some basic troubleshooting tips if you run into problems 
  
* **Ok, what's next?**
  + List of resources to look at if you want to learn more about R

<br>

## A Suggested Workflow

There are multiple ways to successfully complete your figures. Here are a few strategies based on your potential situation:

1. Completely New to R
  * Read completely through the *Getting Started* section and download all the materials you'll need
  * Work through the *Data Analysis Crash Course*
  * Watch the videos and follow along with the code outline
  * Refer back to the **On to University Lake** and **Troubleshooting** sections if you get stuck.

2. Have some R Experience 
  * Read completely through the *Getting Started* section and download all the materials you'll need
  * Skim the *Data Analysis Crash Course* section
  * Watch the videos and follow along with the code outline
  * Refer back to the **On to University Lake** and **Troubleshooting** sections if you get stuck.

3. Decided to start this the night before 
  * Make some coffee or drink a Red Bull
  * Read completely through the *Getting Started* section and download all the materials you'll need
  * Watch the videos (maybe at 1.5x speed) and follow along with the code outline
  * If you get stuck, refer back to this guide (see **Troubleshooting**, **On to University Lake**)
  
<br>

# Getting Started 

<br>

## Materials You'll Need 

* **Computer**  
  + Ideally one that runs Windows, macOS, or Linux. You can make a Chromebook work for what we're doing but will take a little more effort. 
  + Ideally *your* computer. It's helpful to know that your files will be in the same place you left them (and to know R and R package versions will be the same). This isn't 100% necessary - and you will be able to finish everything regardless - but working on your own computer is definitely a proactive approach to avoid issues down the road. 

* **Code Outline & Example Data** 
  + Found on [**Canvas**](https://canvas.iu.edu/lms-prd/gateway) 
      * Two example datasets: `water-chemisty.csv` and `plankton.csv`. Download and save in your project folder.
      * Code outline: `limno-workshop-student.Rmd`. Outlines the code used to make your figures (follows the video lectures). Download and save in your project folder.
  
* **Video lectures**
  + All lecture videos are hosted on YouTube [**here**](https://www.youtube.com/playlist?list=PL7WBQwZeFrDKuFnYZh0Cd0RWN_8E77g61)
      * [**Importing and Manipulating the Data**](https://youtu.be/ivPcpSDi6kQ) 
      * [**Making the Water Chemistry Figures**](https://youtu.be/Ih4jBnhzvaY)
      * [**Making the Plankton Figures**](https://youtu.be/aTvm74tmS0g)
      * [**Making the Light Figure**](https://youtu.be/l0T4na1us-c)
  
* **Written Materials**
  + Found on [**Canvas**](https://canvas.iu.edu/lms-prd/gateway)
      * `limno-workshop.pdf` or `limno-workshop.html`: contains everything covered in the video lectures, plus more.
      * `Required Figures for Lab Reports`: examples of all the figures you need to make

<br>

## Install R and RStudio

We will use the open-source programming language [**R**](https://www.r-project.org/) for this workshop. It's free (Yay!) and relatively easy to install on your own computer. We'll use [**RStudio**](https://rstudio.com/) to access R. You have several options to set up R and RStudio on your computer:

* **R & R Studio on your computer (recommended)**
    * R can be installed by:
        1. Go to [**CRAN**](https://cran.r-project.org/), the Comprehensive R Archive Network
        2. Select the `Download R for` link that is appropriate for your computer
        3. Download the latest release by clicking the corresponding link
        4. Double click on the downloaded file (check `Downloads`). Follow the prompts to install.
        5. If you are using macOS, you'll also need to install XQuartz [**here**](https://www.xquartz.org/).
    
    * RStudio can be installed by: 
        1. Go to [**RStudio**](https://rstudio.com/products/rstudio/download/)
        2. Click the `Download` button under the free, open-source license of **RStudio Desktop**
        3. Download the file that is appropriate for your computer, open the downloaded file, and follow the prompts. 

  * If you already have R and/or R Studio installed, I **highly recommend** you re-install the most recent version of both. If you don't want to do that, make sure you update all CRAN packages with the following command: 
    ```{r update-pkgs, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
    update.packages(ask = FALSE, checkBuilt = TRUE)
    ```
  
* **RStudio Cloud**
  + While R is free and widely supported, sometimes it can be a headache to install and configure. If you would like to avoid these potential headaches (or have a Chromebook and/or not using you own computer), [**RStudio Cloud**](https://rstudio.cloud) allows you to run a full instance of RStudio in your browser. There's a generous free tier that allows you to do everything without installing anything! All you have to do is set up a free account [**here**](https://rstudio.cloud/plans/free).  
  
<br> 

## Packages 
 
R has thousands of packages that enhance the capabilities of R. You'll need to install several:     

* [**tidyverse**](https://www.tidyverse.org/): A collection of R packages for data science
* [**here**](https://here.r-lib.org/): To help with file paths 
* [**rmarkdown**](https://rmarkdown.rstudio.com/index.html): To create reproducible analyses
* [**palmerpenguins**](https://allisonhorst.github.io/palmerpenguins/): An example dataset
* [**patchwork**](https://patchwork.data-imaginist.com/)
  
  <br>
  
**How to install:**

  1. **Open RStudio**. 

  2. **Install packages**. On the command line (`>`) on the left of the screen, type the following commands:
```{r install-pkgs, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
install.packages("rmarkdown", dependenices = TRUE)
install.packages("tidyverse", dependencies = TRUE)
install.packages("here", dependencies = TRUE)
install.packages("palmerpenguins")
```
   
  + **Optional packages**: You may want to eventually knit R Markdown documents to PDF. To do so install the `tinytex` by:
    ```{r install-tinytex, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
    install.packages("tinytex")
    
    # Once installed, run:
    tinytex::install_tinytex()
    ```
  
<br>

## Creating a Working Directory and R Project

A working directory is simply the folder on your computer where all your coding project files live. It is also one of the most important things to set up and keep safe. Create a folder on your computer where you normally keep your files. You're not going to want to move this folder once you make it so chose wisely. As for the name, make it short, all lowercase, and do not have spaces. Names like *code*, *r-work*, *code-work* would all be good choices.

Once you have a working directory set up it's time to create a project. In RStudio:

1. Go to *File > New Project*
2. Select *New Directory > New Project*
3. Name the folder (remember all lowercase, no spaces. limnology would probably be a good idea)
4. Select the file path that goes to your working directory folder
5. Click *Create Project*

There should now be a new folder in your working directory with whatever you named your project. In that folder, there will be a file called *your_project_name.Rproj*. If you open that file, R will open a new RStudio session that starts at your project root. 
    
I also recommend creating a folder in your project folder called *data*. This is where all your raw data files should live.

***

\newpage

<br>

# Data Analysis Crash Course

<br>

## First, some basics

```{r include=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
```

### R as a calculator 

At its most basic, R is a calculator:
```{r intro-calculator}
1 + 9 
1 / 200 * 30
(59 + 73 + 2) / 3 
3 ^ 4
```
R also supports all the basic log and trig functions you can find on a scientific calculator...
```{r intro-log-trig, warning = FALSE, message = FALSE}
sqrt(2)
sin(pi / 2)
log(4) # natural log
log10(4) # common log
```
\newpage

Note that we used functions here. R has a bunch of built-in functions for you to use. We can also define our own. If you want to see what a function does, simply put a question mark in front of it. Any information about the function will show up in the bottom right under the **Help** tab.
```{r intro-help, eval=FALSE}
?log()
```

<br>

### Objects in R

Essentially everything we create in R is an object. Objects can be anything from numbers to figures to dataframes. To create an object, use the `<-` operator:
```{r intro-object}
x <- 3 * 4
```

Note that if you run this code, nothing shows up. However, if you look under the **Environment** tab (top right) there should be a value for the object `x`. To view the object, we can simply call the object name directly:
```{r intro-object-call}
x
```
We can assign more than numbers to objects. In this case, we can make a character string:
```{r intro-object-string}
my_string <- "R is kinda cool"
```

We can do math with objects:
```{r intro-object-math}
x * 2
```
Or we can do math with multiple objects
```{r intro-objects-multi}
one_fish <- 1
two_fish <- 2
one_fish + two_fish
```
And finally, we can make new objects from other objects
```{r intro-objects-new}
three_fish <- one_fish + two_fish
three_fish
```

\newpage

<br>

### Combining values with `c()`

Combining values into a vector is an essential function in R.  Vectors are the building block of dataframes and R is very good at doing vector math.  As you get more advanced in R, vectors will constitute the majority of you analysis.  

To create a vector, we can use the `c()` function.  We won't go over every data type that a vector can support, but you'll mostly come across vectors with numeric or character data. Let's make a vector of numeric values between 1-10, called `a_numeric_vector`: 
```{r intro-vectors}
a_numeric_vector <- c(1:10)
a_numeric_vector
```

We can also easily create a character vector using `c()`:
```{r intro-char-vectors}
a_char_vector <- c("this", "is", "a", "character", "vector")
a_char_vector 
```
<br>

### Creating and Importing dataframes 

Most of the data you will come across data stored in things like Excel spreadsheets and comma-separated values files (ie. csv). R allows you to import a variety of file types with ease. You may also run into times where it's just easier to create a dataframe directly in R. We'll cover how to do both. 

We'll go over how to import data in two forms: Excel (.xlsx) and comma-separate values file (.csv). To import a .csv file, we can use the `read_csv()` function in the `readr` package. `readr` already comes with the `tidyverse` library so no need to install anything else! Remember to use the `here` package to help with file paths:
```{r intro-read-csv, eval=FALSE}
library(tidyverse)
library(here)

my_dataframe <- read_csv(here("folder/with/the/data", "my_awesome_data.csv"))
```

If you want to import Excel files, you'll need to install the package `readxl`. Remember that packages are installed with `install.packages()` and to only install a package once (hint: do it directly on the command line)
```{r intro-readxl, eval=FALSE}
install.packages("readxl", dependencies = TRUE)
```

After `readxll` is installed, a similar strategy to `read_csv` can be employed with `read_excel()`:
```{r intro-readxl-use, eval=FALSE}
library(readxl)

my_dataframe <- read_excel(here("folder/with/the/data", "my_awesome_excel_data.xlsx"))
```

\newpage 

Let's move on to creating your own dataframes in R. Say you have some catch data from a pond you surveyed with the species names, how many you collected, and how long your surveyed for. You have two options to make a dataframe directly in R. We'll be using the `tibble` package in the `tidyverse`.  

+ **Create vectors for each variable and then combine with `tibble()`:**
    ```{r intro-tibble}
    species <- c("Largemouth bass", "Bluegill", "Central stoneroller")
    catch_n <- c(50, 15, 60)
    effort_min <- c(10, 10, 10)
    
    my_fish_dataframe <- tibble(species, catch_n, effort_min)
    
    my_fish_dataframe
    ```

+ **Create a dataframe all at once with `tribble()`**
    ```{r intro-tribble}
    my_fish_dataframe <- 
      tribble(
        ~species, ~catch_n, ~effort_min,
        "Largemouth bass", 50, 10,
        "Bluegill", 15, 10,
        "Central stoneroller", 60, 10
      )
    
    my_fish_dataframe
    ```
<br>

### The Mighty %>%

Using a pipe (`%>%`) in R is an incredibly powerful tool. The `%>%` allows you to write code that reads "left to right" instead of from "inside out".  Technically, the `%>%` is found in the `magrittr` package but is ready for use from the `tidyverse`.

To show how pipes work, let's look at a comparison on how code looks without and with a pipe.  Let's first create a vector of random values and call it x.  Note the use of the `c()` (combine) function here:
```{r intro-pipe, warning = FALSE, message = FALSE}
x <- c(0.109, 0.359, 0.63, 0.996, 0.515, 0.142, 0.017, 0.829, 0.907)
```

\newpage

Now let's say that we want to exponentiate each value in `x`, sum those values, and then round them to a whole number.  The pipe-less way of doing so would look like this:
```{r intro-no-pipe ,eval = TRUE, echo = TRUE, warning = FALSE, message = FALSE}
round(sum(exp(x)))
```
Using a pipe allows us to take these functions and start a logical progression from left (start) to right (finish)
```{r pipe-example, eval = TRUE, echo = TRUE, warning = FALSE, message = FALSE}
x %>% 
  exp() %>% 
  sum() %>% 
  round()
```
<br> 

## Penguins!

Before we take a look at the University Lake data, let's get some practice working with actual data in R. We're going to use the `palmerpenguins` package we installed earlier. This package contains two datasets with size measurements for three penguin species in the Palmer Archipelago, Antarctica. 

### Required packages 

You'll need to load several package before we move on to analyzing the penguin data. Remember the `library()` function loads packages into your current session. So let's load the packages we'll need:
```{r pens-pkgs, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(palmerpenguins)
library(patchwork)
```

<br>

### Taking a look at the data 

Normally we would have to load the data separately. However, the penguins data we are after is actually automatically loaded into our environment when we loaded the package `palmerpenguins`.

The next step in a typical data analysis workflow is to take a look at the data you are working with. It's important to remember that data can be large. The term "big data" gets thrown around a lot in the data science world. A simple definition that I like is that if the computer you are using crashes when you try to work with your dataset, then your data are indeed large. Obviously this means that "big data" is a highly subjective term. 

\newpage 

Because of this, it's helpful to inspect your data without committing all of it to your computers memory. R gives you a few options to do that. Let's first look at the structure that the penguin data are in. We can do so by calling the `glimpse()` function from the `dplyr` package that came installed with the `tidyverse`:
```{r pens-glimpse, echo=TRUE, eval=TRUE}
glimpse(penguins)
```
We can see that our dataset is organized in row and columns containing size measurements for several penguin species. The handy thing about `glimpse()` is that it also tells you the dimensions of the dataset (344 rows X 8 columns) and lists all of the data types for each column.

We can view *all* of the data using the `View()` function. When you call `View()`, it will open a new tab in your window where you can view the entire dataset.
```{r pens-view, echo=TRUE, eval=FALSE}
View(penguins)
```

Another useful set of functions to get a quick look at your data are the `head()` and `tail()` functions. These allow you to look at the first and last rows that are in your dataset. Let's take a look at the first rows in the *penguins* dataset:
```{r pens-head}
head(penguins)
```

\newpage

You can customize the number of rows that are returned using the `n = ` argument within `head()` or `tail()`. For example, let's say we wanted to see the last 10 rows:
```{r pens-tails}
tail(penguins, n = 10)
```

<br>

### Counting some penguins 

We saw from both `head()` and `tail()` that we have penguin measurements for different species and islands. It would be helpful for our analysis to get an idea of how individuals are distributed across species and islands. We can quickly do this using the `count()` function to see how may individuals there are per species:
```{r pens-count}
penguins %>%
  count(species)
```

We can go one step further to see how many individuals there are per species per island: 
```{r pens-count-all}
penguins %>%
  count(species, island, .drop = FALSE)
```

\newpage 

You may want to arrange the number of each in either ascending or descending order. We can do this with `arrange()` and `desc()` 
```{r pens-arrange-asc}
# Ascending order 
penguins %>%
  count(species, island, .drop = FALSE) %>% 
  arrange(n)
```
```{r pens-arrange-desc}
# Descending order 
penguins %>%
  count(species, island, .drop = FALSE) %>% 
  arrange(desc(n))
```

<br>

### Group and summarize the species 

It's common to want to calculate some descriptive statistics from our data. Many times we want to do so by some form of a categorical variable. For the penguins dataset, it would make sense to look at our data by species. 

\newpage 

The combination of `group_by()` and `sumamrize()` in the `dplyr` package allows for us to do this with ease. Let's first determine the mean values for *bill_length_mm* and *bill_depth_mm*:
```{r pens-summarize, message=FALSE, warning=FALSE}
penguins %>% 
  group_by(species) %>% 
  summarize(across(bill_length_mm:bill_depth_mm, mean, na.rm = TRUE))
```

Notice the use of `across()`. This function allows us to apply functions across a set of columns with one function rather than having to apply one-by-one. It saves us from having to type a lot more for the same thing:
```{r pens-across, eval=FALSE, message=FALSE, warning=FALSE}
penguins %>% 
  group_by(species) %>% 
  summarize(
    bill_length_mm = mean(bill_length_mm, na.rm = TRUE),
    bill_depth_mm = mean(bill_depth_mm, na.rm = TRUE)
  )
``` 

Another quick note. The `na.rm = TRUE` argument is critical here. You may have saw a few *NA* values in the data. This means that there are missing values where the *NA's* are. R does not know how to do math with missing values (makes sense) and many functions will throw an error if you don't tell them to ignore them. `na.rm = TRUE` tells functions to ignore missing values when performing some sort of calculation.  

We can also summarize columns based on their data types. Let's calculate the mean of all numeric values in the penguins data:
```{r pens-summarize-all, message=FALSE, warning=FALSE}
penguins %>%
  group_by(species) %>%
  summarize(across(where(is.numeric), mean, na.rm = TRUE))
```

\newpage 

The *year* variable happens to be numeric. It doesn't really make sense to take the mean of a series of years. We can use `select()` to drop this column:
```{r pens-select, message=FALSE, warning=FALSE}
penguins %>%
  group_by(species) %>%
  summarize(across(where(is.numeric), mean, na.rm = TRUE)) %>% 
  select(-year)
```

Up until now, we've only applied summarize by a single function. Most of the time we want to calculate multiple descriptive statistics. Luckily `across()` accomplishes this with ease. 
```{r pens-across-multi, message=FALSE, warning=FALSE}
penguins %>%
  group_by(species) %>%
  summarize(across(bill_length_mm, list(mean, median, min, max, sd), na.rm = TRUE)) 
```

Well that worked but do you see the problem here? What the heck does *bill_length_mm_1* mean? We can infer that it corresponds to the first function we included in `list()`, mean. However, we aren't in the business of creating vague column variables. We can use some R magic to rename the columns based off of the function we're using:
```{r pens-across-names, message=FALSE, warning=FALSE}
penguins %>%
  group_by(species) %>%
  summarize(across(bill_length_mm, 
                   list(mean = mean, median = median, min = min, max = max, sd = sd), 
                   .names = "{col}_{fn}", na.rm = TRUE)) 
```

\newpage 

<br>

### Transforming columns 

Another typical task when working with data is to manipulate column variables. The `mutate()` function allows us to either manipulate existing variables or create new ones.

Let's say we need to convert the body mass measurements we took from grams to kilograms, but keeping both variables:
```{r pens-mutate, message=FALSE, warning=FALSE}
penguins %>% 
  mutate(
    body_mass_kg = body_mass_g / 1000
  )
```

That was easy! We can see that a new column, `body_mass_kg`, was created based on the existing `body_mass_g` column. Now R isn't great about significant figures and you'll need to be very aware of them when you're calculating additional variables. We can use the `round()` function to easily correct for this. Let's say we only want one decimal place in `body_mass_kg`. We can round one of two ways. We can either round when we calculate the variable or after it is calculated:
```{r pens-round-during, eval=FALSE}
# During 
penguins %>% 
  mutate(
    body_mass_kg = round(body_mass_g / 1000, 1)
  )

# After
penguins %>% 
mutate(
  body_mass_kg = body_mass_g / 1000, 
  body_mass_kg = round(body_mass_kg, 1)
)

```

\newpage 

<br>

### Transforming everything

Sometimes we need to reshape our data depending on the format the dataframe is in. Both `pivot_longer()` and `pivot_wider()` allow for this. Our current penguin dataframe is in a *wide* format. Say that we want to create a new variable *measurement* that includes the measurement variables and then a `value` variable to contain the value:
```{r pens-pivot-long}
penguins %>% 
  pivot_longer(cols = bill_length_mm:body_mass_g, 
               names_to = "measurement", 
               values_to = "value")
```

We can see that `pivot_longer()` gathered all of the measurement variables into `measurement` and the values into `value`. This type of transformation wouldn't be a good idea with the penguin data but `pivot_longer()` can be useful in other cases. We'll see how it's handy when working with our profile data.

<br>

### Selecting the penguins you want

One final thing before we move on to visualizing the penguin data. It is common to want to filter specific values in a dataframe. The `filter()` function in `dplyr` allows us to quickly do this. Let's say we only want penguins that were measured on Dream Island. We can filter the penguins dataframe with one line of code:
```{r pens-filter-single}
penguins %>% 
  filter(island == "Dream")
```

\newpage 

We can also filter by multiple matching objects. If we wanted to only include specific penguin species we could do so by using the `%in%` operator:
```{r pens-filter-multi, eval=FALSE}
penguins %>% 
  filter(species %in% c("Chinstrap", "Gentoo"))
```

Sometimes it makes sense to filter *out* something. We know that we only have three species so instead of filtering *for* two species, we can simply filter *out* the one we don't want by adding `!` in front of the column we're filtering:
```{r pens-filter-out, eval=FALSE}
penguins %>% 
  filter(!species == "Adelie")
```

\newpage 

<br>

### Making a basic figure with `ggplot2`

Now that we have a pretty good idea of what's in the penguins data, let's visualize some of it. The `ggplot2` package is by far the most popular plotting package in R. `ggplot2` uses the *Grammar of Graphics* as the underlying theory to make figures. We're not going to get into the theory but know that making figures in `ggplot2` consists of adding *layers* to a plot until you have the final product. 

The first step to make any figure is to tell `ggplot2` what data you intend to use. There are multiple ways to do this but I prefer to use the pipe operator and pass the data to the `ggplot()` function like so:
```{r pens-plot-data}
penguins %>% 
  ggplot()
```
\newpage 

You'll notice that all we see is a blank rectangle. This is because we have not mapped the specific variables we intend to use in the `penguins` dataset. We will do this by mapping the x and y-variables with `aes()`. Let's say we're interested in the relationship between *bill_length_mm* and *bill_depth_mm*:
```{r pens-plot-aes}
penguins %>% 
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm))
```
\newpage 

Ok, so this is still not what we're after. We are seeing the variables mapped to the x and y-axis but we're not seeing any points. Remember that `ggplot2` uses layers to build a figure. We need to add points using `geom_point()`:
```{r pens-plot-point, warning=FALSE, message=FALSE}
penguins %>% 
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(size = 2)
```
Now we're seeing some data. Congrats, you made your first figure with ggplot! Now let's try to find something meaningful to plot!

\newpage

<br>

### Map penguins differently 

With ggplot, we can map additional variables to aesthetics like shape and color. Let's try to take our last figure and map each species of penguin to a different color:
```{r pens-plot-color, warning=FALSE, message=FALSE}
penguins %>% 
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(aes(color = species), size = 2)
```
\newpage 

Ok, now we're seeing some clustering between species just by adding some color to our figure! Let's go one step further and also change the shape by species:
```{r pens-plot-shape, warning=FALSE, message=FALSE}
penguins %>% 
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(aes(color = species, shape = species), size = 2)
```
One really important thing to note. See how we mapped the species of penguin to color and shape in `geom_point()`, but not with size? This is because we put the color and shape arguments in the `aes()` function and did not do that for size. Instead, we included size outside of the `aes()` to apply a style to all of the points in `geom_point()`, not to a specific variable. 

\newpage 

<br>

### Add some style to your plot 

* **Color** 
    * We can modify colors of the points in the last plot by manually defining the colors with `scale_color_manual`:
    ```{r pen-plot-point-colors, warning=FALSE, message=FALSE}
    penguins %>% 
      ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
      geom_point(aes(color = species, shape = species), size = 2) +
      scale_color_manual(values = c("darkorange","darkorchid","cyan4"))
    ```

\newpage 

* **Scales** 
    * We can also modify the scale of the x and y-axis by adding the following:
    ```{r pen-plot-point-scales, warning=FALSE, message=FALSE}
    penguins %>% 
      ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
      geom_point(aes(color = species, shape = species), size = 2) +
      scale_color_manual(values = c("darkorange","darkorchid","cyan4")) +
      coord_cartesian(xlim = c(30, 60), ylim = c(10, 25)) +
      scale_x_continuous(breaks = seq(30, 60, 10)) +
      scale_y_continuous(breaks = seq(10, 25, 5))
    ```

\newpage

* **Labels** 
    * We can modify the the axis labels by:
    ```{r pen-plot-point-labs, warning=FALSE, message=FALSE}
    penguins %>% 
      ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
      geom_point(aes(color = species, shape = species), size = 2) +
      scale_color_manual(values = c("darkorange","darkorchid","cyan4")) +
      coord_cartesian(xlim = c(30, 60), ylim = c(10, 25)) +
      scale_x_continuous(name = "Bill Length (mm)", breaks = seq(30, 60, 10)) +
      scale_y_continuous(name = "Bill Depth (mm)", breaks = seq(10, 25, 5))
    ```

\newpage 

* **Themes** 
    * `ggplot2` provides full customization of styling the plot. We'll go into more detail about themes when we work with the University Lake data. Here are a few built-in themes available: 
    ```{r pen-plot-theme-light, warning=FALSE, message=FALSE, echo=FALSE,  fig.width=7, fig.height=7}
    p1 <- penguins %>% 
      ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
      geom_point(aes(color = species, shape = species), size = 2) +
      scale_color_manual(values = c("darkorange","darkorchid","cyan4")) +
      coord_cartesian(xlim = c(30, 60), ylim = c(10, 25)) +
      scale_x_continuous(name = "Bill Length (mm)", breaks = seq(30, 60, 10)) +
      scale_y_continuous(name = "Bill Depth (mm)", breaks = seq(10, 25, 5)) +
      theme_light() + 
      ggtitle("theme_light")
    
    p2 <- penguins %>% 
      ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
      geom_point(aes(color = species, shape = species), size = 2) +
      scale_color_manual(values = c("darkorange","darkorchid","cyan4")) +
      coord_cartesian(xlim = c(30, 60), ylim = c(10, 25)) +
      scale_x_continuous(name = "Bill Length (mm)", breaks = seq(30, 60, 10)) +
      scale_y_continuous(name = "Bill Depth (mm)", breaks = seq(10, 25, 5)) +
      theme_linedraw() +
      ggtitle("theme_linedraw")

    p3 <- penguins %>% 
      ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
      geom_point(aes(color = species, shape = species), size = 2) +
      scale_color_manual(values = c("darkorange","darkorchid","cyan4")) +
      coord_cartesian(xlim = c(30, 60), ylim = c(10, 25)) +
      scale_x_continuous(name = "Bill Length (mm)", breaks = seq(30, 60, 10)) +
      scale_y_continuous(name = "Bill Depth (mm)", breaks = seq(10, 25, 5)) +
      theme_minimal() +
      ggtitle("theme_minimal")

    (p1 | p2) / p3
    ```
    
\newpage 

<br>

### Some additional plots 

`ggplot2` supports more geoms than we can cover in this guide. But here are a few that you may frequently come across in the future:

* **Histogram**
    ```{r pen-plot-histogram, warning=FALSE, message=FALSE}
    penguins %>% 
      ggplot( aes(x = flipper_length_mm)) +
      geom_histogram(aes(fill = species), alpha = 0.5, position = "identity") +
      scale_fill_manual(values = c("darkorange","purple","cyan4")) +
      labs(x = "Flipper length (mm)", y = "Frequency", title = "Penguin flipper lengths") +
      theme_minimal() 
    ```

\newpage 

* **Boxplot**
    ```{r pen-plot-boxplot, warning=FALSE, message=FALSE}
    penguins %>% 
      ggplot(aes(x = species, y = flipper_length_mm)) +
      geom_boxplot(aes(color = species), width = 0.3, show.legend = FALSE) +
      scale_color_manual(values = c("darkorange","purple","cyan4")) +
      labs(x = "Species", y = "Flipper length (mm)") +
      theme_minimal() 
    ```

We'll cover everything else you need to know about `ggplot2` when we work with the University Lake data!

***

\newpage 

# On to University Lake 

<br>

## Required Libraries 

We'll first need to load the packages we installed earlier using `library()`:
```{r}
library(tidyverse)
library(here)
library(patchwork)
```
<br>

## Importing the data 

Our first task is to load the University Lake data into our environment. Since our data are stored as comma-separated values (ie. .csv file), we'll use `here()` to tell R where to find the files and `read_csv()` to import them. 

The basic form of what this will look like is: 
```{r eval=FALSE, echo=TRUE}
object_name_for_r <- read_csv(here("file/path/to/folder/with/data", "name_of_data_file.csv"))
```

In this case, both data files are stored in the folder **data**. So loading those data into the current environment is done with:
```{r import-data, echo=TRUE, eval=TRUE, message=FALSE, echo=FALSE}
water_chem_raw <- read_csv(here("data", "water_chemistry.csv"))
plankton_raw <- read_csv(here("data", "plankton.csv"))
```

If everything worked, two dataframes will appear in the **Environment** tab in the top right named `water_chem_raw` and `plankton_raw`.

<br>

## Working with the water chemistry data 

<br>

### Taking a look 

It's a good idea to first get an idea of what format the data are in and what's there. Let's first look at the overall structure with `glimpse()`:
```{r wtr-chem-glimpse, eval=TRUE, echo=TRUE}
glimpse(water_chem_raw)
```

Remember you can view the *entire* dataset with `View(water_chem_raw)`, the first 6 rows with `head(water_chem_raw)`, or the last 6 rows with `tail(water_chem_raw)`

We can see that in `water_chem_raw`, each row is a unique depth measurement and each column is either a explanatory variable (e.g. lake_name, sample_type) or a parameter we intend to plot.

<br>

###  Control for MDL's 

The next step in preparing the water chemistry data for plotting is to control for MDLs, or method detection limits. We'll use the `mutate()` function to override the existing variables to control for the MDLs (Note that you should verify what the current MDLs are and then update the following code accordingly):
```{r mlds, eval=TRUE, echo=TRUE}
water_chem_clean <- water_chem_raw %>% 
  mutate(
    srp_mgl = ifelse(srp_mgl <= 0.002, 0.002, srp_mgl),
    tp_mgl = ifelse(tp_mgl <= 0.002, 0.002, tp_mgl),
    no3_mgl = ifelse(no3_mgl <= 0.009, 0.009, no3_mgl),
    nh3_mgl = ifelse(nh3_mgl <= 0.015, 0.015, nh3_mgl),
    tn_mgl = ifelse(tn_mgl <= 0.104, 0.104, tn_mgl)
  )
```

<br>

### Calculating Error 

Another task we need to complete is to determine the error indicated by either the duplicate and replicate measurements. We'll first create a custom function to find the error and then apply this function to `water_chemistry_clean`:
```{r meas-error, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# Create custom function for error 
error_calc <- function(x){
  abs(x - lag(x))
} 

# Apply function to data 
water_chem_error <- water_chem_clean %>% 
  group_by(depth) %>% 
  mutate(across(temp_c:chla_ugl, error_calc)) %>% 
  ungroup(depth) %>% 
  filter(sample_type %in% c("rep", "dup")) %>% 
  rename_at(vars(-lake_name, -depth, -sample_type), 
            funs(paste0(., sep = "_", "error"))) %>% 
  select(-lake_name, -sample_type)
```

\newpage 

<br>

### Average values 

Now that the error between measurements is calculated, we can average the measurements by depth to get the final points for our figures:
```{r avg-depths, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
water_chem_clean <- water_chem_clean %>% 
  group_by(depth) %>% 
  summarize(across(temp_c:chla_ugl, mean, na.rm = TRUE))
```

<br>

### Organic Nitrogen

Another variable we need to calculate for our figures is organic nitrogen. We will first define the function `get_orgn()`based off of TN, NO3, and NH3. After that, we can apply to the dataframe to create a new column, `orgn_mgl`
```{r orgn, eval=TRUE, echo=TRUE}
get_orgn <- function(tn_mgL, no3_mgL, nh3_mgL){
    org_n <- tn_mgL - (no3_mgL + nh3_mgL)
    return(org_n)
}

# Apply function to df 
water_chem_clean <- water_chem_clean %>% 
  mutate(orgn_mgl = round(get_orgn(tn_mgl, no3_mgl, nh3_mgl), 3))
```

<br>

### Percent Light Level 

Percent light level is another variable we'll need to put our figures together. Again, we'll define a function and then apply it to the existing dataframe with `mutate()`:
```{r per-light, eval=TRUE, echo=TRUE}
# Create function
get_percent_light <- function(light_at_depth, light_at_surface){
  
  percent_light <- round((light_at_depth / light_at_surface) * 100, 1)
  return(percent_light)
}

# Apply to data 
water_chem_clean <- water_chem_clean %>% 
  mutate(
    light_level_per = get_percent_light(light_dep_mmol, light_sur_mmol)
  )
```

\newpage 

<br>

### One Percent Light 

The last variable we need to calculate is the one percent light level. However, we'll create a separate object rather than adding to the dataframe since the value applies to the entire profile:
```{r one-per-light, eval=TRUE, echo=TRUE}
# Create function
get_one_percent <- function(depths, light){

  # Determine surface and one percent light
  surface_light <- light[[1]]
  one_percent <- surface_light * 0.01

  # Remove zeros and determine length
  light1 <- light[!light %in% 0]
  len <- length(light1)

  # Make depth vector sample length
  depths1 <- depths[1:len]

  # Calculate one percent light level
  mod <- lm(depths1 ~ log(light1))
  coef <- coef(mod)
  int <- coef[1]
  slope <- coef[2]
  one_percent_light_level <- slope * log(one_percent) + int

  return(one_percent_light_level)
}

# Apply to data 
one_percent_light <- round(
                      get_one_percent(
                        water_chem_clean$depth, 
                        water_chem_clean$light_dep_mmol
                      ), 1)
```

\newpage 

<br>

### Sig Figs 

One thing we need to control for are significant figures. This is easy to do with `mutate()` and `round()`:
```{r sig-figs, eval=TRUE, echo=TRUE}
water_chem_clean <- water_chem_clean %>% 
  mutate(
    temp_c = round(temp_c, 2), 
    do_mgl = round(do_mgl, 2),
    do_sat_per = round(do_sat_per, 1), 
    cond_umhos = round(cond_umhos, 1),
    light_sur_mmol = round(light_sur_mmol, 0),
    light_dep_mmol = round(light_dep_mmol, 0),
    ph = round(ph, 1), 
    alk_mgl = round(alk_mgl, 0),
    turb_ntu = round(turb_ntu, 1),
    srp_mgl = round(srp_mgl, 3),
    tp_mgl = round(tp_mgl, 3),
    nh3_mgl = round(nh3_mgl, 3),
    no3_mgl = round(no3_mgl, 3),
    tn_mgl = round(tn_mgl, 3),
    chla_ugl = round(chla_ugl, 2),
    orgn_mgl = round(orgn_mgl, 3),
    light_level_per = round(light_level_per, 1)
  )
```

<br>

### Combining with error 

Since we saved the error values as a separate dataframe, we want to join them back with the cleaned water chemistry data. We'll use `left_join` to do this:
```{r combine-df, echo=TRUE, eval=TRUE}
water_chem_clean <- water_chem_clean %>% left_join(water_chem_error, by = c("depth"))
```

<br>

### Secchi 

Similar to the one percent level, Secchi depth is separate from our water chemistry data and needs to be defined:
```{r secchi, eval=TRUE, eval=TRUE}
secchi_m <- 0.75
```

<br>

### Bottom of Epilimnion

Like Secchi and one percent light, the bottom of the epilimnion needs to be defined separately. We don't have a function for this one. Use Figure 6-3 in Wetzel (pg. 76) to estimate.
```{r epi-bottom, eval=TRUE, echo=TRUE}
bottom_of_epi <- 1.5
```

***

\newpage 

<br>

## Working with the plankton data 

We've already imported the plankton data as `plankton_raw`. Let's get an idea of what those data look like: 
```{r plankton-head, eval=TRUE, echo=TRUE}
head(plankton_raw)
```

We can see that the data are organized similar to the water chemistry data with the plankton taxa organized by column. 

<br>

### Defining the cube root function 

To make the phytoplankton figure more readable, we want to transform the NU/L values using cube-root. R does not include a cube-root function out of the box, but we can easily define one:
```{r cube-root, eval=TRUE, echo=TRUE}
cube_rt <- function(x){
  x ^ (1/3)
}
```

\newpage

<br>

### Tranforming the plankton data 

The first step to transform the plankton data is to calculate the minimum, maximum, and mean values for each taxa per depth. Now, we could filter out each taxa and apply these functions to each dataframe. However, that would require a lot of copy-and-paste. Instead, we can apply those functions to all columns with `summarize()` and `across()` and then collect each taxa into the same column:
```{r plank-transform, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
plankton_summary <- plankton_raw %>% 
  group_by(depth) %>% 
  summarize(across(
    ends_with("nul"), 
    list(mean = mean, min = min, max = max), 
    .names = "{col}_{fn}")) %>% 
  pivot_longer(
    cols = dolichospermum_nul_mean:chaoborus_nul_max,
    names_to = c("taxa", "unit", "stat"),
    names_sep = "_" 
  ) %>% 
  mutate(taxa = str_to_title(taxa)) %>% 
  select(-unit) %>% 
  pivot_wider(
    names_from = stat,
    values_from = value
  )
```

<br>

### Phytoplankton data

To prep the phytoplankton data, we need to apply the cube-root function and then determine the upper and lower bounds for the error bars: 
```{r phyt-data, eval=TRUE, echo=TRUE}
phyts <- plankton_summary %>% 
  filter(taxa %in% c("Dolichospermum", "Aphanizomenon", "Microcystis", "Ceratium")) %>% 
  mutate(
    mean_rt = cube_rt(mean),
    upper_rt = cube_rt(max),
    lower_rt = cube_rt(min),
    upper_bound = abs(mean_rt - upper_rt),
    lower_bound = abs(mean_rt - lower_rt)
  ) %>% 
  select(depth, taxa, mean_rt, upper_bound, lower_bound)

```

<br>

### Zooplankton data

We don't have to apply the cube-root function to the zooplankton taxa. However, we need to determine the upper and lower bounds:
```{r zoo-data, eval=TRUE, echo=TRUE}
zoops <- plankton_summary %>% 
  filter(!taxa %in% c("Dolichospermum", "Aphanizomenon", "Microcystis", "Ceratium")) %>% 
  rename(lower_bound = min, upper_bound = max)
```

***

<br>

## Making the Water Chemistry Figures 

All of the water chemistry figures follow the same format. Once you get the first figure completed, you can easily use it as a template for the others. There are ways to avoid having to copy and paste so much (ie. make a function) but that's a little beyond the scope of this workshop. Feel free to experiment with creating custom plotting functions if you're feeling adventurous. 

One quick note about how we are formatting the figures. You may notice in the limnology literature that many profile figures include multiple parameters with multiple scales on the same figure. While this is common, there are a few reasons why we are not doing this:

* Combining multiple parameters with *different* scales makes interpretation difficult. This increase in complexity makes you increasingly reliant on different line/point patterns, legends, and colors to explain your figure. Subplots allow you to avoid all of this and make the interpretation clear for the reader.

* ggplot2 does not natively support multiple scales on the same plot for the reason I listed below. It can be done with base graphics if you want to do this in the future

* ggplot2 is considerably easier to use than base graphics. ggplot2 is also widely used in the scientific community and a highly desirable skill to have. 

\newpage 

<br>

### Figure 1 - Mapping the data 

Check out the *Required Figures for Lab Reports* document for what Figure 1 is supposed to look like. We essentially need to make a separate figure for each parameter and then combine them together for the final product. We're going to inspect each element of the first subplot separately just to get an idea of how things work. After that, we can skip a lot of steps for the remaining subplots. 

We're going to focus on the temperature subplot and the first step is to map the data. Specifically, we're going to define the x and y-variables with `aes()`:
```{r fig1-data}
water_chem_clean %>% 
  ggplot(aes(x = temp_c, y = depth))
```

\newpage 

<br>

### Figure 1 - Points and Lines 

Now that we have the data we want mapped, we can start adding the points and lines with `geom_`:
```{r fig1-points}
water_chem_clean %>% 
  ggplot(aes(x = temp_c, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) 
```

\newpage 

<br>

### Figure 1 - Adding error bars 

Our next step is to add the error bars. Remember, that we've already defined the error values so now we just need to call them:
```{r fig1-error, warning=FALSE, message=FALSE}
water_chem_clean %>% 
  ggplot(aes(x = temp_c, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, 
                     xmin = temp_c - temp_c_error, 
                     xmax = temp_c + temp_c_error), 
                 height = 0.1)
```
\newpage 

<br>

### Figure 1 - Adding annotations 

We want to add the Secchi depth measurement to our figure. This is a two step process with `geom_segment()` for the line and `geom_text()` to label the line:
```{r fig1-annotate, warning=FALSE, message=FALSE}
water_chem_clean %>% 
  ggplot(aes(x = temp_c, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, 
                     xmin = temp_c - temp_c_error, 
                     xmax = temp_c + temp_c_error), 
                 height = 0.1) +
  geom_segment(aes(x = 15.5, 
                   y = secchi_m, 
                   xend = 19.5, 
                   yend = secchi_m), 
               size = 1, linetype = "dashed") +
  geom_text(aes(x = 17.5, y = 0.15), label = "Secchi depth (0.75 m)", size = 3)
```

\newpage 

<br>

### Figure 1 - Scale and labels 

You've probably noticed that the current plot looks weird. It's because we haven't flipped the y-axis so the surface measurement is at the top of the plot. We also want to control the scale of the x-axis for the labels and then change the axis labels. We can do all of that with only three lines of code:
```{r fig1-scale, warning=FALSE, message=FALSE}
water_chem_clean %>% 
  ggplot(aes(x = temp_c, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, 
                     xmin = temp_c - temp_c_error, 
                     xmax = temp_c + temp_c_error), 
                 height = 0.1) +
  geom_segment(aes(x = 15.5, 
                   y = secchi_m, 
                   xend = 19.5, 
                   yend = secchi_m), 
               size = 1, linetype = "dashed") +
  geom_text(aes(x = 17.5, y = 0.15), label = "Secchi depth (0.75 m)", size = 3) +
  coord_cartesian(xlim = c(9, 26)) +
  scale_y_reverse(name = "Depth (m)") +
  scale_x_continuous(name = "Temperature (°C)")
```

\newpage 

<br>

Now that looks better! Last step is to save the plot as an object:
```{r fig1-save-example}
p_temp <- water_chem_clean %>% 
  ggplot(aes(x = temp_c, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, 
                     xmin = temp_c - temp_c_error, 
                     xmax = temp_c + temp_c_error), 
                 height = 0.1) +
  geom_segment(aes(x = 15.5, 
                   y = secchi_m, 
                   xend = 19.5, 
                   yend = secchi_m), 
               size = 1, linetype = "dashed") +
  geom_text(aes(x = 17.5, y = 0.15), label = "Secchi depth (0.75 m)", size = 3) +
  coord_cartesian(xlim = c(9, 26)) +
  scale_y_reverse(name = "Depth (m)") +
  scale_x_continuous(name = "Temperature (°C)")
```

<br>

### Figure 1 - Remaining subplots 

So we're done with one out of the four subplots we need. The remaining subplots follow the same pattern as the temperature plot and are completed for you in the `limno-workshop-student` file.

```{r fig1-subplots, include=FALSE, warning=FALSE, message=FALSE}
# DO subplot
p_do <- water_chem_clean %>% 
  ggplot(aes(x = do_mgl, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, xmin = do_mgl - do_mgl_error, xmax = do_mgl + do_mgl_error), height = 0.1) +
  geom_text(aes(x = 8, y = 1.5), label = "1% Light Level (2.1 m)", size = 3) +
  geom_segment(aes(x = 6, y = one_percent_light, xend = 10, yend = one_percent_light), size = 1, linetype = "dotted") +
  coord_cartesian(xlim = c(0, 16)) +
  scale_y_reverse(name = "") +
  scale_x_continuous(name = "Dissolved Oxygen (mg/L)", breaks = seq(0, 16, 4))

# Turbidity subplot 
p_turb <- water_chem_clean %>% 
  ggplot(aes(x = turb_ntu, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, xmin = turb_ntu - turb_ntu_error, xmax = turb_ntu + turb_ntu_error), height = 0.1) +
  coord_cartesian(xlim = c(10, 40)) +
  scale_y_reverse(name = "Depth (m)") +
  scale_x_continuous(name = "Turbidity (NTU)", breaks = seq(10, 40, 5))

# DO SAT subplot 
p_dosat <- water_chem_clean %>% 
  ggplot(aes(x = do_sat_per, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, xmin = do_sat_per - do_sat_per_error, xmax = do_sat_per + do_sat_per_error), height = 0.1) +
  coord_cartesian(xlim = c(0, 200)) +
  scale_y_reverse(name = "") +
  scale_x_continuous(name = "D.O. Saturation (%)", breaks = seq(0, 200, 40)) 
```

<br>

### Figure 1 - Combining and adjusting 

Now that we have all of the subplots completed all we have to do is combine all the plots and apply a theme to the plot. Be careful to make sure you swap out the `+` and `&` sign when you're putting the figure together:
```{r fig1-theme, warning=FALSE, message=FALSE}
plot1 <- (p_temp | p_do) / (p_turb | p_dosat) + 
  plot_annotation(tag_levels = "A", tag_suffix = ".") &
  theme_bw() &
  theme(
    panel.grid = element_blank(), 
    panel.border = element_rect(color = "black"),
    axis.text = element_text(color = "black"),
    axis.ticks = element_line(color = "black")
  )
```

<br>

### Figure 1 - Saving

Finally, we can save our figure with `ggsave()`. For Windows users, make sure you add the `type = "cairo` to adjust for some weird rendering. Also watch the `width = ` and `height = ` arguments if the plots come out with funky dimensions:
```{r fig1-save, eval=FALSE, warning=FALSE, message=FALSE}
ggsave(plot1, file = "figure1.png", device = "png", type = "cairo", width = 7, height = 7)
```

<br>

### Making Figures 2-5

Figures 2 through 5 are really similar to Figure 1 in terms of code. Make sure you check out the `limno-workshop-student` file. Every plot has a scaffold that outlines every function you will need to create the remaining plots. Refer to the **Required Figures for Lab Reports** on what the layout and labels should look like. 
 
***

<br>

## Making the Plankton Figures 

The code required to do the plankton figures is very similar to the water chemistry figures we've already made. We're again going to do through the first subplot in detail and then apply those concepts to the remaining subplots. 

While we only went over the first water chemistry plot (Figure 1), we're going to go over how to do both the phytoplankton (Figure 6) and zooplankton (Figure 7) figures.

<br>

### Figure 6 - Filtering 

Remember the `filter()` verb? We can use this to filter out the taxa we want: 
```{r fig6-filter, eval=FALSE}
phyts %>% 
  filter(taxa == "Aphanizomenon")
```

<br>

### Figure 6 - Mapping the data 

Now we can pass the filtered data to `ggplot()`, map the variables, and then add the appropriate geoms:
```{r fig6-geoms, eval=FALSE}
phyts %>% 
  filter(taxa == "Aphanizomenon") %>% 
  ggplot(aes(x = mean_rt, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75)
```

\newpage 

<br>

### Figure 6 - Error bars & Lines

The next step is to add the error bars and draw lines to represent the one percent light level and the bottom of the epilimnion:
```{r fig6-lines}
phyts %>% 
  filter(taxa == "Aphanizomenon") %>% 
  ggplot(aes(x = mean_rt, y = depth)) + 
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, 
                     xmin = mean_rt - lower_bound, 
                     xmax = mean_rt + upper_bound), 
                 height = 0.1) +
  geom_hline(aes(yintercept = one_percent_light), linetype = "dashed") +
  geom_hline(aes(yintercept = bottom_of_epi), linetype = "dotted")
```

\newpage 

<br>

### Figure 6 - Scale and labels 

One thing that is unique to the phytoplankton figure is that we need to directly modify the axis labels. We can do that within `scale_x_continuous()`:
```{r fig6-scale}
p_aphani <- phyts %>% 
  filter(taxa == "Aphanizomenon") %>% 
  ggplot(aes(x = mean_rt, y = depth)) + 
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, 
                     xmin = mean_rt - lower_bound, 
                     xmax = mean_rt + upper_bound), 
                 height = 0.1) +
  geom_hline(aes(yintercept = one_percent_light), linetype = "dashed") +
  geom_hline(aes(yintercept = bottom_of_epi), linetype = "dotted") +
  coord_cartesian(xlim = c(0, 60)) +
  scale_y_reverse(name = "Depth (m)") +
  scale_x_continuous(
    name = "",
    breaks = seq(0, 60, 20),
    labels = c(0, expression(20^3), expression(40^3), expression(60^3))) +
  ggtitle("Aphanizomenon") 
```

<br>

### Figure 6 - Remaining subplots 

The remaining subplots look a lot like the last figure we made. The only wrinkle is that on the far right subplot you need to add the labels for the one percent light and bottom of epilimnion lines. The code for the remaining subplots is completed for you in `limno-workshop-student`
```{r fig6-remaining, include=FALSE}
p_ceratium <- phyts %>% 
  filter(taxa == "Ceratium") %>% 
  ggplot(aes(x = mean_rt, y = depth)) + 
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, xmin = mean_rt - lower_bound, xmax = mean_rt + upper_bound), height = 0.1) +
  geom_hline(aes(yintercept = one_percent_light), linetype = "dashed") +
  geom_hline(aes(yintercept = bottom_of_epi), linetype = "dotted") +
  coord_cartesian(xlim = c(0, 60)) +
  scale_y_reverse(name = "") +
  scale_x_continuous(
    name = "",
    breaks = seq(0, 60, 20),
    labels = c(0, expression(20^3), expression(40^3), expression(60^3))) +
  ggtitle("Ceratium") 

p_dolicho <- phyts %>% 
  filter(taxa == "Dolichospermum") %>% 
  ggplot(aes(x = mean_rt, y = depth)) + 
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, xmin = mean_rt - lower_bound, xmax = mean_rt + upper_bound), height = 0.1) +
  geom_hline(aes(yintercept = one_percent_light), linetype = "dashed") +
  geom_hline(aes(yintercept = bottom_of_epi), linetype = "dotted") +
  coord_cartesian(xlim = c(0, 60)) +
  scale_y_reverse(name = "") +
  scale_x_continuous(
    name = "",
    breaks = seq(0, 60, 20),
    labels = c(0, expression(20^3), expression(40^3), expression(60^3))) +
  ggtitle("Dolichospermum") 

p_microcystis <- phyts %>% 
  filter(taxa == "Microcystis") %>% 
  ggplot(aes(x = mean_rt, y = depth)) + 
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  geom_errorbarh(aes(y = depth, xmin = mean_rt - lower_bound, xmax = mean_rt + upper_bound), height = 0.1) +
  geom_text(aes(x = 42, y = 1.9), label = "1% Light Level (2.1 m)", size = 3) +
  geom_hline(aes(yintercept = one_percent_light), linetype = "dashed") +
  geom_text(aes(x = 42, y = 1.3), label = "Bottom of Epi. (1.5 m)", size = 3) +
  geom_hline(aes(yintercept = bottom_of_epi), linetype = "dotted") +
  coord_cartesian(xlim = c(0, 60)) +
  scale_y_reverse(name = "") +
  scale_x_continuous(
    name = "",
    breaks = seq(0, 60, 20),
    labels = c(0, expression(20^3), expression(40^3), expression(60^3))) +
  ggtitle("Microcystis") 
```

<br>

### Figure 6 - Combining and adjusting 

Now that we have the four subplots completed, we can combine the subplots, apply the theme, and save:
```{r fig6-combine}
p_phyt <- (p_aphani| p_ceratium | p_dolicho | p_microcystis) +
  plot_annotation(caption = "Density (#/L)") & 
  theme_bw() & 
  theme(plot.caption = element_text(hjust = 0.5, size = 12, vjust = 8),
        plot.title = element_text(hjust = 0.5, size = 10, face = "italic"),
        axis.title.y = element_text(size = 12),
        panel.grid = element_blank(), 
        panel.border = element_rect(color = "black"),
        axis.text = element_text(color = "black"),
        axis.ticks = element_line(color = "black"))

ggsave(p_phyt, file = "figure6.png", device = "png", type = "cairo", height = 6, width = 10)
```

<br>

### Figure 7 - Zooplankton

The zooplankton figure is very similar to the phytoplankton figure. Check out the `limno-workshop-student` and the corresponding video to complete the figure. 

\newpage 

<br>

## Making the Light Figure 

The light figure is a bit different than what we've done with the previous figures. However, it's going to take considerably less steps to complete (Yay!). No need for subplots here, but there are some wrinkles that you'll need to look out for as we put the light figure together. 

<br>

### Figure 8 - Data, Points, and Lines 

Just like the other figures, we'll map the data and add the geoms:
```{r fig8-data, message=FALSE, warning=FALSE}
water_chem_clean %>% 
  ggplot(aes(x = light_level_per, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) 
```

\newpage 

<br>

### Figure 8 - Scales 

To make the figure easier to interpret, we'll log transform the x-axis with `scale_x_log10`:
```{r fig8-scales, message=FALSE, warning=FALSE}
water_chem_clean %>% 
  ggplot(aes(x = light_level_per, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  coord_cartesian(xlim = c(0.1, 100), clip = 'off') +
  scale_y_reverse(name = "Depth (m)") +
  scale_x_log10(name = "Light Transmittance (%)", 
                breaks = c(0.1, 1, 10, 100), 
                labels = c("0.01", "1", "10", "100")) 
```

\newpage 

<br>

### Figure 8 - Annotatations and arrows 

The next step is to add the annotation and arrow. This takes a little guess-and-check to get the positioning right:
```{r fig8-annotate, message=FALSE, warning=FALSE, fig.width = 4, fig.height=4}
water_chem_clean %>% 
  ggplot(aes(x = light_level_per, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  coord_cartesian(xlim = c(0.1, 100), clip = 'off') +
  scale_y_reverse(name = "Depth (m)") +
  scale_x_log10(name = "Light Transmittance (%)", 
                breaks = c(0.1, 1, 10, 100), 
                labels = c("0.01", "1", "10", "100")) +
  annotation_logticks(sides = "b") +
  geom_label(label= expression('Z'['1%']*' = 2.1m'), aes(x = 3.1, y = 3.25),
  label.padding = unit(0.55, "lines"), label.size = 0.35, color = "black", fill="white") +
  geom_segment(
    aes(x = 1.15, xend = 1.7, y = 2.1, yend = 2.7),
    arrow = arrow(ends = "first", type = "open", length = unit(0.25, "cm"))
  )
```

\newpage 

<br>

### Figure 8 - Themes

Finally, we'll apply the theme and save the final figure
```{r fig8-theme, message=FALSE, warning=FALSE}
p_light <- water_chem_clean %>% 
  ggplot(aes(x = light_level_per, y = depth)) +
  geom_point(size = 1.5) + 
  geom_path(size = 0.75) + 
  coord_cartesian(xlim = c(0.1, 100), clip = 'off') +
  scale_y_reverse(name = "Depth (m)") +
  scale_x_log10(name = "Light Transmittance (%)", 
                breaks = c(0.1, 1, 10, 100), 
                labels = c("0.01", "1", "10", "100")) +
  annotation_logticks(sides = "b") +
  geom_label(label= expression('Z'['1%']*' = 2.1m'), aes(x = 3.1, y = 2.75),
  label.padding = unit(0.55, "lines"), label.size = 0.35, color = "black", fill="white") +
  geom_segment(
    aes(x = 1.15, xend = 1.7, y = 2.1, yend = 2.7),
    arrow = arrow(ends = "first", type = "open", length = unit(0.25, "cm"))
  ) +
  theme_bw() +
  theme(
    panel.grid = element_blank(), 
    panel.border = element_rect(color = "black"),
    axis.text = element_text(color = "black"),
    axis.ticks = element_line(color = "black")
  )

ggsave(p_light, file = "figure8.png", device = "png", type = "cairo")
```

***

\newpage 

<br>

# Troubleshooting Tips 

* **Read!** 
    * You'd be surprised how much an error message tells you. Things like forgetting to load functions and missing parentheses are easy to determine from the error message. 

* **Refresh a lot**
    * The old adage of "Did you turn it off and turn it back on again?" applies to R as well. Sometimes restarting the session will do you wonders. Simply save your work and go to *Session > Restart R*. Another option is to close RStudio entirely, reopen the project, and start fresh with a new environment.

* **When in doubt, Google it!**
    * If you are stuck on a error, the next logical step is to copy the error and paste in your web browser. Most likely someone else has experienced that error and has a solution. The most popular source for these solutions is by far [stackoverflow](https://stackoverflow.com/)
    
* **Getting help from RStudio** 
    * RStudio does a lot to try to help you with your R problems. There are numerous cheatsheets you can download by going to *Help > Cheatsheets*. RStudio also maintains a super helpful support site [here](https://support.rstudio.com/hc/en-us/articles/200552336-Getting-Help-with-R)

* **Send me an email** 
    * If you have tried the above tips and are still having issues, feel free to email me with your questions. Just make sure you include the error and the code that is giving you issues. My email is <**coryjsauve@gmail.com**>.
  
***

\newpage 

<br>

# Ok, what's next?

## Good things to read

If you feel motivated to continue to learn R here are some (mostly) free online resources I highly recommend checking out: 

* [*R for Data Science*](https://r4ds.had.co.nz/)
* [*Advanced R*](https://adv-r.hadley.nz/)
* [*R Packages*](https://r-pkgs.org/)
* [*Introdory Fisheries Analyses with R*](http://derekogle.com/IFAR/)
* [*Fundamentals of Data Visualization*](https://clauswilke.com/dataviz/)
* [*Data Visualization: A Practical Introduction*](https://socviz.co/)
* [*Text Mining with R*](https://tidytextmining.com)
* [RStudio Education Blog](https://education.rstudio.com/blog/)
* [R Studio Blog](https://blog.rstudio.com/)

## Good people to follow 

One of the best things about R is that there is an amazing community behind the language. Here are some really great Twitter follows/blogs to check out:

* Hadley Whickham: @hadleywickham (https://hadley.nz)
* Jenny Bryan: @JennyBryan; (https://jennybryan.org/)
* Andrew Heiss: @andrewheiss; (https://www.andrewheiss.com/)
* David Robinson: @drob; (http://varianceexplained.org/)
* Emily Robinson: @robinson_es; (https://hookedondata.org/)
* will Chase @Will_R_Chase; (https://www.williamrchase.com/)
* Julia Silge: @juliasilge; (https://juliasilge.com/)
* Danielle Navarro: @djnavarro; (https://djnavarro.net/)
* Thomas Lin Pedersen: @thomasp85; (https://www.data-imaginist.com/)
* Mine Cetinkaya-Rundel: @minebocek; (https://mine-cr.com)
* Jacqueline Nolis: @skyetetra; (https://jnolis.com)
* Allison Horst: @allison_horst; (https://www.allisonhorst.com/)
* Kieran Healy: @kjhealy; (https://kieranhealy.org/)
* Max Kuhn: @topepos; (http://appliedpredictivemodeling.com/)
* Jordan S Read: @jordansread
* Claus wilke: @ClausWilkel (https://clauswilke.com/)
* Mara Averick: @dataandme
* Winston Change: @winston_chang
* Zev Ross: @zevross
* Studio @rstudio
* R-bloggers: @Rbloggers
* Tom Mock: @thomas_mock

***







